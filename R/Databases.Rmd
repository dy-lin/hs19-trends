---
title: "Databases Webscraping"
author: "Raissa Philibert"
date: "October 18, 2019"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```


```{r libraries}
# https://github.com/ropensci/fulltext
library(fulltext)
library(pubchunks)
library(tidyverse)
library(magrittr)
library(dplyr)
library(purrr)
library(here)
###custom functions
source(here("./R/getDOI.R"))
```

```{r API keys}
Sys.setenv(ELSEVIER_SCOPUS_KEY = '3da3a9477a6e0a7fefc0bbfb49f288f5', SPRINGER_KEY = 'dc293398c185c33d930865ae1fd6eae3', ENTREZ_KEY = "1621b309cd4dab23f28c432d4e7baa76d909" )

```

```{r import}

searchTerms <- read.csv(here("/raw-data/SearchTerms.csv"))
searchTerms$Term %<>% as.character()
searchTerms <- searchTerms %>% filter(Topic=="Databases")
```


##Selecting relevant databases for fulltext 
###The format of ENTREZ data output is not compatible with downstream analyses. 

###Microsoft academic search 
APU key = yes (requires an Asure account and a credit card).Not used. 
Rate limit: 10,000 per month, 1 per second

###Scopus 
API key= yes
lim = 20,000 (default 25)
Boolean syntax (AND, OR, AND NOT)
Scopus_loop function
date = year of publication

###Euro PMC
lim = NA (default 25)
pubYear = year of publication


###BioRxiv
only records with CrossRef are retained
lim = NA (default 10)
deposited = yyyy-mm-dd date of deposit

###aRxiv
lim = NA (default:10)

All of the above are not necessary. To query publication data PMC and Crossref would be sufficicent, and still contain redundencies.  


```{r webscrape genbank}

databaseList <- ft_search_ls()

# ft_links() - get links for articles (xml and pdf).
resFound <- NULL

for(st in searchTerms$Term) {
  
  res1 <- ft_search(query = st, from = databaseList, limit = 10)
  
  for(nm in names(res1)){
    Found <- res1[[nm]][["found"]]
    if(!is.null(Found)){
      resFound <- bind_rows(resFound,data.frame(db=nm,found=Found,searchTerm=st))}
  }
  
}

```

We will be getting DOIs from plos, crossref and bmc
```{r getDOI}
plosLinks <- res1$plos$ids
limit = max(resFound[resFound$db %in% c("bmc","crossref"),"found"])
res1$crossref$data %>% View()
```

```{r crossref}
offsets <- seq(1,limit,1000)
st <- "Gene expression omnibus"
crossref <- purrr::map_df(offsets,function(x) crossrefsearch(x,st=st))

save(crossref,file=here("/data/crossref_geo_results.Rda"))
```

```{r getLinks}
mylinks <- ft_links(res1)$plos$ids

# ft_get() - get full or partial text of articles.
x <- ft_get(mylinks)


x %>% ft_collect() %>% 
  pub_chunks("publisher") %>% 
  pub_tabularize()

x %>%
  ft_collect() %>% 
  pub_chunks(c("doi", "refDois", "history", "journal_meta", "publisher", "author", "aff", "title", "keywords", "abstract")) %>%  
  pub_tabularize() -> hold
  # .$elife

```



```{r standardFormat}

df_raw <- NULL
for (nm in names(hold)){
  
  df_raw <- bind_rows(df_raw, map_df(hold[[nm]], `[`))
  
  }

df <- data.frame(doi=character(),
                 refDois=character(),
                 Year=numeric(),
                 Journal=character(),
                 Author=character(),
                 Institution=character(),
                 Title=character(),
                 Keywords=character(),
                 Abstract=character())

```

```{r save}

write.csv(df_raw, here("/data/YourName-YourTopic.csv"))

```


